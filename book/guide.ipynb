{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from wordfreq import zipf_frequency\n",
    "\n",
    "\n",
    "# load data '../../chars.csv'\n",
    "chars = pd.read_csv('../汉字.csv', sep=',', encoding='utf-8')\n",
    "words = pd.read_csv('../词汇.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataframe\n",
    "df = pd.DataFrame(columns=['page', 'chars', 'words',\n",
    "                  'level', 'pinyin', 'part_of_speech'])\n",
    "\n",
    "page = 0\n",
    "for index, row in chars.iterrows():\n",
    "\n",
    "    page += 1\n",
    "\n",
    "    df.loc[index, 'page'] = page\n",
    "    df.loc[index, 'chars'] = row['汉字']\n",
    "    df.loc[index, 'level'] = row['级别']\n",
    "    df.loc[index, 'pinyin'] = row['拼音']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['freq'] = ''\n",
    "for page, row in df.iterrows():\n",
    "    frequency = zipf_frequency(row['chars'], 'zh')\n",
    "    df.loc[page, 'freq'] = frequency\n",
    "\n",
    "# sort by frequency in descending order\n",
    "df = df.sort_values(by=['freq'], ascending=False)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['old_chars'] = ''\n",
    "for page, row in df.iterrows():\n",
    "    cum_chars = df.loc[:page, 'chars'].str.cat(sep=', ')\n",
    "    df.at[page, 'old_chars'] = cum_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words(df, words, num_chars_interval=(2, 10), max_page=-1):\n",
    "    for page, row in df.iterrows():\n",
    "        chars = row['old_chars']\n",
    "        contains = words['词语'].apply(lambda x: all(\n",
    "            char in chars for char in x) and num_chars_interval[0] <= len(x) <= num_chars_interval[1])\n",
    "\n",
    "        # filter the found words in the same level\n",
    "        # contains &= words['级别'] == row['level']\n",
    "\n",
    "        df.at[page, 'words'] = words[contains]['词语'].values.tolist()\n",
    "        # remove duplicates\n",
    "        df.at[page, 'words'] = list(set(df.at[page, 'words']))\n",
    "\n",
    "        # print(\n",
    "        #     f\"Page {row['page']}:\\n\\tChars: {chars}\\n\\tWords: {len(df.at[page, 'words'])}\\n\")\n",
    "\n",
    "        # stop at max_page\n",
    "        if row['page'] == max_page:\n",
    "            break\n",
    "\n",
    "\n",
    "find_words(df, words, num_chars_interval=(2, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the order\n",
    "level_order = ['一级', '二级', '三级', '四级', '五级', '六级', '高等']\n",
    "\n",
    "# Sort the dataframe\n",
    "df['level'] = pd.Categorical(df['level'], level_order)\n",
    "df.sort_values(['level', 'freq'], inplace=True, ascending=[True, False])\n",
    "\n",
    "df.groupby('level').count()\n",
    "\n",
    "# Save to csv\n",
    "# df.to_csv('guide.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df and 'old_chars' is a correct DataFrame with relevant columns set up properly.\n",
    "# Convert 'words' column to a list of words if it is a string representation.\n",
    "if isinstance(df.at[0, 'words'], str):\n",
    "    # Update this split according to actual delimiter.\n",
    "    df['words'] = df['words'].apply(lambda x: x.split(', '))\n",
    "\n",
    "# Assume 'old_chars' is separated by comma and space as in the provided code.\n",
    "if 'old_chars' in df.columns:\n",
    "    # Update this split according to actual delimiter.\n",
    "    df['old_chars'] = df['old_chars'].apply(lambda x: x.split(', '))\n",
    "else:\n",
    "    # If 'old_chars' column does not exist, create empty lists.\n",
    "    df['old_chars'] = [[] for _ in range(len(df))]\n",
    "\n",
    "# Create a cumulative set of old characters introduced up to each row.\n",
    "cumulative_old_chars = set()\n",
    "df['cumulative_old_chars'] = pd.Series(dtype='object')\n",
    "for i in range(len(df)):\n",
    "    cumulative_old_chars.update(df.at[i, 'old_chars'])\n",
    "    df.at[i, 'cumulative_old_chars'] = cumulative_old_chars.copy()\n",
    "\n",
    "# Define a function to find new words\n",
    "\n",
    "def find_new_words(row):\n",
    "    return [word for word in row['words'] if row['chars'] in word and word not in row['cumulative_old_chars']]\n",
    "\n",
    "\n",
    "# Apply the function to get new_words\n",
    "df['new_words'] = df.apply(find_new_words, axis=1)\n",
    "\n",
    "# sort new_words by frequency\n",
    "df['new_words'] = df['new_words'].apply(\n",
    "    lambda x: sorted(x, key=lambda y: zipf_frequency(y, 'zh'), reverse=True))\n",
    "\n",
    "# Reorder the columns\n",
    "df2 = df[['level', 'chars', 'pinyin', 'new_words']]\n",
    "\n",
    "# Save to CSV\n",
    "df2.to_csv('guide.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save every level (as numbers) to csv\n",
    "for i, level in enumerate(level_order, 1):\n",
    "    df2[df2['level'] == level].to_csv(\n",
    "        f'guide_{i}.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocabulary = pd.read_csv('../词汇.csv', sep=',', encoding='utf-8')\n",
    "guide_words = []\n",
    "\n",
    "for i, level in enumerate(level_order, 1):\n",
    "    temp_vocab = vocabulary[vocabulary['级别'] == level]\n",
    "    temp_vocab = temp_vocab[temp_vocab['词语'].str.len() >= 2]\n",
    "    df = pd.read_csv(f'guide_{i}.csv', sep=',', encoding='utf-8')\n",
    "    guide_words.extend(df['new_words'])\n",
    "\n",
    "    guide_words = [word.strip('[]').strip(\"'\") for word in guide_words]\n",
    "    guide_words = list(filter(None, guide_words))\n",
    "\n",
    "    print(f\"Guide {level}: {len(guide_words)}\")\n",
    "    print(f'Vocab {level}: {len(temp_vocab.values.tolist())}')\n",
    "\n",
    "    # print(set(guide_words))\n",
    "\n",
    "    # not_in_vocab = set(temp_vocab['词语'].values.tolist()) - set(guide_words)\n",
    "\n",
    "    # print(f\"not in vocab: {len(not_in_vocab)}\")\n",
    "\n",
    "    # find all words not present in the vocabulary\n",
    "    not_in_guide = set(guide_words) - set(temp_vocab['词语'].values.tolist())\n",
    "    print(f\"not in guide: {len(not_in_guide)}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide_words = pd.read_csv(f'guide.csv', sep=',', encoding='utf-8')\n",
    "vocabulary = pd.read_csv('../词汇.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "guide_words = guide_words['new_words'].values.tolist()\n",
    "guide_words = [word.strip('[]').strip(\"'\") for word in guide_words]\n",
    "guide_words = list(filter(None, guide_words))\n",
    "\n",
    "print(f\"Guide: {len(guide_words)}\")\n",
    "print(f'Vocab: {len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find same char but different pinyin\n",
    "homophones = df2[df2.duplicated(subset=['chars'], keep=False)]\n",
    "# sort by level and chars\n",
    "homophones.sort_values(['level', 'chars'], inplace=True)\n",
    "homophones.to_csv('homophones.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
